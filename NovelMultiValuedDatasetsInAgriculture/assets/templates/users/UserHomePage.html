{%extends "users/userbase.html" %}
{%load static%}
{%block contents%}

<style>
    h1 {
        margin-left: 35%;
    }
</style>
<div>
    <h1> Agriculture
    </h1>
    </br>
    <p><strong>In the modern era, many reasons for agricultural plant disease
            due to unfavorable weather conditions. Many reasons that
            influence disease in agricultural plants include variety/hybrid
            genetics, the lifetime of plants at the time of infection,
            environment( soil, climate), weather (temperature, wind, rain,
            hail, etc), single versus mixed infections, and genetics of the
            pathogen populations. Due to these factors, diagnosis of plant
            diseases at the early stages can be a difficult task.</strong></p>
    <h2>Naive Bayes:</h2>
    <p><strong>Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to
            problem instances, represented as vectors of feature values, where the class labels are drawn from some
            finite set.
            There is not a single algorithm for training such classifiers, but a family of algorithms based on a common
            principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the
            value of any other feature, given the class variable.
            For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter.
            A naive Bayes classifier considers each of these features to contribute independently to the probability
            that this fruit is an apple, regardless of any possible correlations between the color, roundness, and
            diameter features.</strong></p>

    <h2>Multilayer perceptron:
    </h2>
    <p><strong>A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). The term MLP is
            used ambiguously, sometimes loosely to mean any feedforward ANN, sometimes strictly to refer to networks
            composed of multiple layers of perceptrons (with threshold activation).
            Multilayer perceptrons are sometimes colloquially referred to as "vanilla" neural networks, especially when
            they have a single hidden layer.
            An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer.
            Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a
            supervised learning technique called backpropagation for training.
            Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish
            data that is not linearly separable.</strong></p>



</div>

{%endblock%}